<font face= "宋体">

# 机器学习
## 1.机器学习基本概念
### 目标
1. 从原始数据中提取特征
2. 学习一个映射函数，将特征/原始数据 映射到语义空间
3. 寻找数据与目标间的关系
### 种类
1. 监督学习
   * 给定标签
   * 回归或分类等任务 
2. 无监督学习
   * 无标签
   * 聚类或若干降维任务
3. 强化学习
   * 序列数据决策学习
   * 从环境交互中学习 
### 监督学习
1. 标注数据：贴好标签
2. 学习模型：
3. 损失函数：
   1. 0-1 损失函数
   2. 平方损失函数
   3. 绝对损失函数
   4. 对数损失函数(对数似然)
#### 经验风险
$$\frac{1}{n}\sum_{i=1}^n Loss(y_i,f(x_i))$$
* 泛化能力
* 过学习：
  惩罚项防止模型过于复杂：
  $$\frac{1}{n}\sum_{i=1}^n Loss(y_i,f(x_i))+\lambda J(f)$$
  其中$J(f)$为惩罚项，$\lambda$是惩罚项系数，用来调整惩罚的强度
* 欠学习：还没学好
* 生成方法&判别方法

## 回归分析
* 分析不同变量之间存在关系的研究：回归分析
### 一元线性回归
寻找一条用$y=ax+b$表达的直线，使得这条直线尽可能靠近或穿过所给的数据
最佳回归模型使得残差平方和的平均值$\frac{1}{N}\sum(y-\hat y)^2$最小
$$a=\frac{\sum_{i=1}^n x_iy_i-n\bar{x}\bar{y}}{\sum_{i=1}^n x_ix_i - n\bar{x}^2}$$
$$b=\bar{y}-a\bar{x}$$
### 多元线性回归
通过$m$个训练数据$\{(\vec{x_i},y_i)\}_{i=1}^m$,其中$\vec{x_i}\in \R^D$
找到一组参数$\vec{a}=[a_0,a_1,...,a_D]$，使得
$$f(\vec{x_i})=a_0+\vec{a}^T\vec{x_i}$$
最小化均方误差函数
$$J_m=\frac{1}{m}\sum_{i=1}^m(y_i-f(\vec{x_i}))^2$$

### 逻辑斯蒂回归
#### 梯度下降

## 决策树
* 信息熵
  K个信息，组成样本D，记第k个信息发生的概率为$p_k(1\le k\le K)$，则这K个信息的信息熵为:
  $$E(D)=-\sum_{k=1}^K p_k \log_2 p_k$$
  $E(D)$的值越小，D包含的信息越确定，也称D的纯度越高
* 信息增益
  $$Gain(D,A)=Ent(D)-\sum_{i=1}^n \frac{|D_i|}{|D|}Ent(D_i)$$
  根据信息增益来选择最佳属性来对原样本进行采集划分
  信息增益偏向选择分支多的属性，在一些场合容易过拟合。考虑对分支过多进行惩罚，引入另一个“纯度”的概念，
  简易的度量指标 $Gini(D)=1-\sum_{k=1}^K p_k^2$

## 线性判别分析（linear discriminant analysis）

* 线性判别分析是一种基于监督学习的降维方法，也称为Fisher线性判别分析。
* LDA利用其类别信息，将其线性投影到一个低位空间上，在低位空间中同一类别尽可能靠近，不同类别样本尽可能远离

* 类内方差小，类间间隔大

### 推导

试图将其投影在下面这条直线上
$$
y(\vec {x})=\omega ^T \vec{x} ,\omega \in \R^n
$$

$$
协方差矩阵：\Sigma _i =\Sigma _{x\in X_i}(\vec{x}-\vec{m_i})(\vec{x}-\vec{m_i})^T
$$

$$
s_1=\omega^T\Sigma_1 \omega,s_2=\omega^T\Sigma_2 \omega
$$

为了使类内样本的方差小，则需要最小化$s_1+s_2$取值

样本中心
$$
m_1=w^Tm_1,m_2=w^Tm_2
$$

为了使类间的间隔大，则需最大化$||m_2-m_1||_2^2$的取值

所以定义了**目标函数**
$$
J(w)=\frac{||m_2-m_1||_2^2}{s_1+s_2}=\frac{w^TS_bw}{w^TS_ww}
$$
由于最后的结果只与$w$的方向有关而与大小无关，所以可以建立约束使得分母为1，因此对应的拉格朗日函数
$$
L(w)=w^TS_bw-\lambda (w^TS_ww-1)
$$
分别求偏导之后发现取最小值的条件即为：
$$
S_w ^{-1}S_b \omega =\lambda \omega
$$
取最大的特征根对应的投影方向

即
$$
w=S_w^{-1}(m_2-m_1)
$$
**一般步骤：**

1. 计算每个样本的均值

2. 计算类内散度矩阵$S_w$和类间散度矩阵$S_b$

3. 根据
   $$
   S_w^{-1}S_bW=\lambda W
   $$
   来求解$S_w^{-1}S_b$对应前$r$个最大特征值所对应的特征向量$(w_1,w_2,...,w_r)$构成矩阵$W$

4. 通过矩阵$W$将每个样本映射到低维空间，实现特征降维

### Ada boosting(自适应提升)

#### 核心思想：

对于一个复杂的分类系统，可将其分解为若干个子任务，然后将若干个子任务综合构建到一起，最终完成该复杂任务。每个**弱分类器**(weak classifiers)只能完成任务的部分。将若干个弱分类器组合起来就形成了一个**强分类器**(strong classifier)。

#### 计算学习理论

**霍夫丁不等式**:
$$
P(|x-y|\ge\epsilon)\le2e^{-2N\epsilon^2}
$$
其中$N$为采样人口总数，而$\epsilon$为可容忍的误差

**概率近似正确：**

|          |      |
| :------: | ---- |
| 强可学习 |      |
| 弱可学习 |      |

#### 思路描述

* 核心问题
  * 如何改变训练数据的权重：提高在上一步中错误的权重
  * 如何将一系列弱分类器组合成强分类器

#### 步骤

1. 初始化每个训练样本的权重

2. 训练出$M$个弱分类器

   1. 使用具有分布权重$D_m$的训练数据学习得到m个基分类器

   2. 计算$G_m(x)$在训练数据集上的分类误差
      $$
      err_m=\sum_{i=1}^Nw_{mi}I(G_m(x_i)\not= y_i)
      $$

   3. 计算弱分类器$G_m(x)$的权重
      $$
      \alpha_m=\frac{1}{2}\ln(\frac{1-err_m}{err_m})
      $$
      

   4. 更新下一轮第m+1个分类器训练时第i个训练样本的权重
      $$
      
      $$

3. 1241252

M个弱分类器，则M个弱分类器线性组合所产生的误差满足的条件：
$$
P(\sum_{i=1}^{M}G_m(x)\not=\kesai_x)\le e^{-\frac 12M(1-2\epsilon)^2}
$$
