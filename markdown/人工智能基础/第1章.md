
# 绪论

## 1.1 人工智能的起源

* 1955年8月，四位学者提出Artificial Intelligence的术语
* 机器能像人一样认知、思考和学习，即用计算机模拟人的只能

### 1.1.1 人工智能需要研究的七个方面的问题

1. 自动计算模拟人脑高级功能
2. 使用通用语言进行计算机编程以模仿人脑推理
3. 神经元相互连接形成概念
4. 对计算复杂性的度量
5. 算法自我提升
6. 算法的抽象能力
7. 随机性和创造力

### 1.1.2 人工智能定义

从计算性和智能性的角度而言，人工智能是以机器为载体所实现的人类智能或生物智能

## 1.2 可计算载体

### 1.2.1 形式化系统

#### 1.2.1.1 性质

1. 完备性：所有能从某形式化系统推导出来的知识，都可以从这个形式化系统推导出来
2. 一致性：所有可推导出来的东西不对同时推导出来它的否定
3. 可判定性：对于形式化系统推导得到的任何知识，存在算法可在有限步内判定其为真或为假

#### 1.2.1.2 哥德尔特不完备性定理

* 任何表达力足够强的形式系统，都不可能同时具备一致性和完备性，而且这个系统本身的一致性不能在系统内被证明。

#### 1.2.1.3 图灵机模型

* 凡是可计算的函数都可以以用图灵机计算。

## 1.3 智能计算方法

### 1.3.1 符号主义为核心的逻辑推理

#### 1.3.1.1 推理

#### 1.3.1.1.1 推理的三种方式

1. ##### 归纳推理：从个别事实出发，推演出一般性知识作为结论的推理过程

2. ##### 演绎推理：从一般性前提出发，通过推导，得出具体陈述或个别结论的过程

3. ##### 因果推理：判断事物间存在的原因和结果的关系

#### 1.3.1.1.2 推理难度的三个层次

1. 关联
2. 干预
3. 反事实

### 1.3.2 问题求解为核心的探寻搜索

#### 1.3.2.1 搜索的三种方式

##### 1.3.2.1.1 无信息搜索

* 无信息搜索是盲目搜索
* 广度优先搜索
* 深度优先搜索

##### 1.3.2.1.2 有信息搜索

* 有信息搜索又叫启发式搜索
* 具有辅助信息
* 贪婪最佳优先搜索
* A* 搜索

##### 1.3.2.1.3 对抗搜索

* 也称博弈搜索
* 在一个竞争在环境中，智能体之间通过竞争实现相反的利益
* 一方最大化这个利益，另一方最小化这个利益
* 代表算法：最大最小搜索，Alpha-beta剪枝搜索，蒙特卡洛树搜索

### 1.3.3 数据驱动为核心的机器学习

#### 1.3.3.1 数据驱动

数据驱动是从数据出发，从承载表达某一概念的数据中心直接学习该概念所涉及的模式，然后基于学习得到的模板对未知数据进行分类或识别

#### 1.3.3.2 机器学习算法

##### 1.3.3.2.1 监督学习

* 训练集为标注好的数据，含有样本数据和标注信息

1. 监督学习基于训练数据，从假设空间这一学习范围中学习得到一个**最优映射函数**$f$（也被称为决策函数）

   映射函数$f$将数据映射到予以标注空间，实现数据的分类和识别

   监督学习算法的**目的**是使得$f(x_i)$和$y_i$之间的差值最小

2. 监督学习的另一种方法是通过训练数据**学习概率分布**$P(y_i|x_i)$

   进而通过这一取值来判断书序$x_i$属于$y_i$的概率，以实现对$x_i$的分类和识别

3. 赫布理论认为“神经元之间持续重复经验刺激可导致突触传递效能增加”，这也被认为是**深度学习**的基础

   * 端到端的机制(end-to-end)学习隐含在数据内部的隐含模式
   * 得到更强表达力和泛化能力的特征表达
   * 将深度学习运用提取得到的特征用于识别和分类等任务
   * 实例：
     * 卷积神经网络，卷积层、池化层、激活函数、全链接层、误差反向传播

4. 监督学习算法：

   * 回归分析
   * 提升算法(boosting)
   * 支持向量机
   * 决策树
   * 隐狄利克雷分布（latent dirichlet allocation,LDA）
   * 隐马尔可夫链

##### 1.3.3.2.2 无监督学习

* 无监督学习指数据本身不包含标注信息
* 无监督学习算法：
  * 聚类
  * 降维（主成分分析）
  * 期望最大化算法（expectation maximization,EM）



##### 1.3.3.2.3 半监督学习

指一部分数据有标注信息而一部分数据没有标注信息

### 1.3.4 行为主义为核心的强化学习

#### 1.3.4.1 强化学习

* 人工智能领域的一个主要研究目标是实现完全自主的智能体
* 该智能体能与其所处的环境进行交互根据环境所提供的奖励反馈或惩罚反馈来学习所处状态可市价的最佳行动
* 利用“尝试-试错”与平衡“探索-利用”等机制不断进步，改变行动策略。
* 强化学习是这样一种赋予智能体自监督学习能力，使其能够自主与环境交互，做出决策序列，完成序列化形式的任务，向“学会学习”这一能力塑造目标而努力

#### 1.3.4.2 不同之处

* 强化学习解决的是**序贯决策**优化问题，即智能体与环境不断交互，在某个状态采取某一行为后进入一个新的状态，根据环境给出的奖励或惩罚反馈（reward）来改进策略，以求得最大的累积奖惩（accumulated reward）

#### 1.3.4.3 监督学习、无监督学习、强化学习比较

|          | 监督学习                       | 无监督学习             | 强化学习                                           |
| -------- | :----------------------------- | ---------------------- | -------------------------------------------------- |
| 学习依据 | 基于监督信息                   | 基于对数据结构的假设   | 基于评价(evaluate)                                 |
| 数据来源 | 一次性给定(含标注信息)         | 一次性给定(无标注信息) | 在序列交互中产生，只有一个序列结束后才会明确奖惩值 |
| 决策过程 | 根据标注信息，做出单步静态决策 | 无                     | 根据环境给出的滞后回报，做出序列决策               |
| 学习目标 | 样本空间到高级语义空间的映射   | 同一类数据的分布模式   | 选择能够获取最大收益的状态到动作的映射             |

#### 1.3.4.4 马尔可夫决策过程

马尔可夫决策过程(markov decision process,MDP)刻画了当前状态采取某一行动后进入后续状态，且因为采取了这一行动会从环境获得一定的奖励反馈或惩罚反馈的机制

#### 1.3.4.5 Q学习& 深度Q学习

1. q函数记录了某个状态下采取某一动作能够收到的将离职或成法制
2. Q学习可以构造一个状态-行为效用表格（state-action utility table）
3. 矩阵（表格）的行和列分别代表状态和行为，矩阵对应元素的值代表某一状态下采取某个行为所能够获得的回报
4. 实际环境中，智能体所处的状态数量可能异常庞大，如果一个状态在训练中从未出现过，Q学习是无法处理的，这使得Q学习预测和泛化能力较弱
5. 一个可行的思路就是将q函数参数化，用神经网络来拟合q函数，这就是深度学习和强化学习结合而形成的深度强化学习(deep reinforcement learning, DRL)

### 1.3.5 博弈对抗为核心的决策职能

#### 1.3.5.1 博弈



* 博弈行为：多个带有相互竞争性质的主体，为了达到各自目标和利益，采取的带有对抗性质的行为
* 现代博弈论主要研究博弈行为中最优的对抗策略及其稳定局势，协助对弈者在一定规则范围内寻求最合理的行为方式，推动机器学习从“数据拟合”过程中以求取“最优解”向求取“**均衡解**”为核心的转变
* 纳什均衡是指非合作博弈，均衡解一定存在

## 1.5 小结

1. 经过长期发展，人们已经慢慢意识到，在脑科学和神经科学等尚未弄清人脑功能机理的前提下，难以用机器直接实现人脑功能

2. 经典人工智能理论框架建立在以**递归可枚举**为核心的**演绎逻辑**和**语义描述**基础方法之上

3. 由于**先决条件问题**和**隐性分支问题**，人们难以事先拟定好智能算法能够处理所有的情况

4. 弱人工智能（领域人工智能）与具有自我学习、直觉推理、自适应和能力迁移等特点的通用人工智能存在差距

5. 也与人类所展示的行为由明显去呗，但在特定领域取得成功的研究为人们探索发展新一代人工智能提供了重要的借鉴和新的方法

6. 当前人工智能载体**图灵机**模型能够完成的可计算任务都是**可递归**的（这意味着可枚举和有序）

7. 需要将人的作用或认知模型引入到人工智能同中，形成混合-增强智能形态，这种形态是人工智能或机器智能可行的、重要的成长模式，也是发展人工智能的可行之道。

   